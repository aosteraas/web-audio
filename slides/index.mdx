import { future, highlight } from "@mdx-deck/themes";
import { Image, Notes, Appear, Split, Horizontal } from "mdx-deck";
import { Squares, AudioTag, AudioEvent } from "./components";
import safari from "./assets/safari.png";

export const themes = [future, highlight];

# Audio, Browsers, and You

### It's easy until it isn't

<Squares />

---

# &lt;audio&gt;

<AudioTag />
<Squares />

```html
<audio controls src="path/to/file.wav"></audio>
```

---

# &lt;audio&gt;

<AudioTag />
<Squares />

```html
<audio controls>
  <source src="path/to/file.wav" type="audio/wav" />
  <source src="path/to/file.mp3" type="audio/mp3" />
  <source src="path/to/file.ogg" type="audio/ogg" />
</audio>
```

---

<Squares />

# &lt;audio&gt;

<AudioEvent />

```js
const audio = document.querySelector("audio");
audio.play();
```

---

<Squares />

# React

```jsx
export function AudioEvent() {
  useEffect(() => {
    const button = document.querySelector("button");
    button.addEventListener("click", play);
    return () => button.removeEventListener("click", play);
  }, []);

  const play = () => {
    // returns `HTMLMediaElement`
    const audio = document.querySelector("audio");
    audio.play();
  };

  return (
    <>
      <button>Play</button>
      <audio controls src={bass} />
    </>
  );
}
```

---

<Squares />

# There's no need to render elements at all

---

<Squares />

# Using `new Audio()`

```js
const play = () => {
  // returns `HTMLMediaElement`
  const sound = new Audio("path/to/sound.wav");
  // set current time back to 0 if it's already
  // playing and you want to start again
  sound.currentTime = 0;
  sound.play();
};
```

---

<Squares />

# Use events to control playback

```js
sound.addEventListener("canplaythrough", event => {
  /* the audio is now playable; play it if permissions allow */
  sound.play();
});
```

---

<Squares />

# Using `new Audio()`

```js
const slaps = [
  { name: "closedSlap", audio: new Audio(closedSlap) },
  { name: "fingerSlap", audio: new Audio(fingerSlap) },
  { name: "mutedSlap", audio: new Audio(mutedSlap) },
  { name: "slap", audio: new Audio(slap) },
  { name: "fingerTone", audio: new Audio(fingerTone) }
];

const sound = slaps.find(x => x.name === "slap");
sound.play();
```

---

<Squares />

# this works perfectly fine except in Safari

<img src={safari} />

---

<Squares />

# Web Audio API to the Rescue

---

<Squares />

# it's broad and crazy

- Feed data from `<audio>`, an oscillator, or a stream
- Use effects like reverb, biquad filter, panner, compressor
- Throw your sources through effects
- Send it to output

---

# Create an `AudioContext`

<Squares />

```ts
const ctx: AudioContext = new (window.AudioContext ||
  window.webkitAudioContext)();
```

---

<Squares />

# Loading Dynamically

```js
// map sounds to create many requests with minimum typing
const requests = sounds.map(async s => {
  const r = await fetch(s.src);
  const buffer = await r.arrayBuffer();
  return { type: s.type, buffer };
});
// It's an array of promises so we can await them all completing
const soundData = await Promise.all(requests);
```

At this point we have audio in an `ArrayBuffer`

---

<Squares />

# Parsing to Consume

```ts
// However the data type is `ArrayBuffer`, which is a
// byte array in basically any other language
const parsable = soundData.map(async d => {
  // you need to copy the buffer with `slice` if you
  // want to use it again, else it's 'consumed'
  const sliced = d.buffer.slice(0);

  const audio = await ctx.decodeAudioData(sliced);
  return { type: d.type, audio };
});

// Again we have an array of promises we need to await completing.
const parsed: Parsed[] = await Promise.all(parsable);

// Finally, set these in state for future use.
setParsed(parsed);
```

---

<Squares />

# All good, right?

---

<Squares />

# Wrong

<Appear>
  <img src={safari} />
</Appear>

---

# Safari & Edge are two peas in a pod

<Squares />

```js
// callback syntax: Safari & Edge
AudioContext.decodeAudioData(ArrayBuffer, successCallback, errorCallback);
// promise-based: Firefox & Chrome
AudioContext.decodeAudioData(ArrayBuffer);
```

---

<Squares />

# Polyfill for Safari

```js
if (!window.AudioContext && window.webkitAudioContext) {
  const oldFunc = webkitAudioContext.prototype.decodeAudioData;
  webkitAudioContext.prototype.decodeAudioData = function(
    arraybuffer: ArrayBuffer
  ) {
    return new Promise((resolve, reject) => {
      oldFunc.call(this, arraybuffer, resolve, reject);
    });
  };
}
```

---

<Squares />

# Putting it all together

```js
// Probably instantiate this elsewhere and import/use everywhere
const ctx = new AudioContext();
// get the sound
const audio = getSound(action);
// create a buffer source (necessary each time we play sound)
const source = ctx.createBufferSource();
// connect to the audio destination (output) and play
source.buffer = audio;
// connect the buffer to the AudioContext destination
source.connect(ctx.destination);
// play
source.start();
```
